{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from collections import namedtuple\n",
    "from pathlib import Path\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "HAMS_SIZE = 3672\n",
    "SPAMS_SIZE = 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file_path = \"./enron1/ham/\"\n",
    "\n",
    "ham_tmp = []\n",
    "path = Path(file_path if len(file_path) >= 2 else '.')\n",
    "for path_in in [x for x in path.glob('*.txt') if x.is_file()]:\n",
    "    with path_in.open() as f:\n",
    "        ham_tmp.append(f.read())\n",
    "        \n",
    "file_path = \"./enron1/spam/\"\n",
    "\n",
    "spam_tmp = []\n",
    "path = Path(file_path if len(file_path) >= 2 else '.')\n",
    "for path_in in [x for x in path.glob('*.txt') if x.is_file()]:\n",
    "    with path_in.open(encoding = \"ISO-8859-1\") as f:\n",
    "        spam_tmp.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_docs = []\n",
    "spam_docs = []\n",
    "docs = []\n",
    "\n",
    "analyzedDocument = namedtuple('AnalyzedDocument', 'words tags')\n",
    "for i,text in enumerate(ham_tmp):\n",
    "    text = re.sub(re.compile(\"[!-/:-@[-`{-~]\"), '', text)\n",
    "    text = re.sub(re.compile(\"\\n\"), '', text)\n",
    "    ham_docs.append(analyzedDocument(text.split(),[i]))\n",
    "    docs.append(analyzedDocument(text.split(),[i]))\n",
    "    \n",
    "analyzedDocument = namedtuple('AnalyzedDocument', 'words tags')\n",
    "for i,text in enumerate(spam_tmp):\n",
    "    text = re.sub(re.compile(\"[!-/:-@[-`{-~]\"), '', text)\n",
    "    text = re.sub(re.compile(\"\\n\"), '', text)\n",
    "    spam_docs.append(analyzedDocument(text.split(),[i]))\n",
    "    docs.append(analyzedDocument(text.split(),[i+len(ham_docs)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec(docs, min_count=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "target = []\n",
    "for i in range(HAMS_SIZE + SPAMS_SIZE):\n",
    "    data.append(list(model.docvecs[i]))\n",
    "    if i < HAMS_SIZE:\n",
    "        target.append(0)\n",
    "    else:\n",
    "        target.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc, roc_curve,f1_score\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "from mlxtend.classifier import StackingCVClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ここから学習をする．\n",
    "# clf1:giniを使ったRF\n",
    "# clf2:entropyを使ったRF\n",
    "# clf3:giniを使ったET\n",
    "# clf4:entropyを使ったET\n",
    "# clf5:適当にパラメータを決定したGBDT\n",
    "# clf6:gridSearchでGBDTをがっつり検証した結果\n",
    "# clf7:gridSearchでGBDTをゆるふわ検証した結果\n",
    "# clf8:LightGBMで適当に\n",
    "# clf9:LightGBMをチューニング\n",
    "# clf10:XGBoostで適当に\n",
    "# clf11:XGBoostをチューニング\n",
    "# clf12:KNN\n",
    "# clf13:Naive Bayes\n",
    "# clf14:svc\n",
    "\n",
    "lr = LogisticRegression()\n",
    "clf1 = RandomForestClassifier(n_estimators=100, n_jobs=-1, criterion='gini')\n",
    "clf2 = RandomForestClassifier(n_estimators=100, n_jobs=-1, criterion='entropy')\n",
    "clf3 = ExtraTreesClassifier(n_estimators=100, n_jobs=-1, criterion='gini')\n",
    "clf4 = ExtraTreesClassifier(n_estimators=100, n_jobs=-1, criterion='entropy')\n",
    "clf5 = GradientBoostingClassifier(n_estimators=100,learning_rate=0.1, max_depth=3)\n",
    "clf6 = GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
    "              learning_rate=0.1, loss='deviance', max_depth=2,\n",
    "              max_features=1.0, max_leaf_nodes=None,\n",
    "              min_impurity_split=1e-07, min_samples_leaf=9,\n",
    "              min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "              n_estimators=100000, presort='auto', random_state=None,\n",
    "              subsample=1.0, verbose=0, warm_start=False)\n",
    "clf7 = GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
    "              learning_rate=0.1, loss='deviance', max_depth=2,\n",
    "              max_features=1.0, max_leaf_nodes=None,\n",
    "              min_impurity_split=1e-07, min_samples_leaf=9,\n",
    "              min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "              n_estimators=3000, presort='auto', random_state=None,\n",
    "              subsample=1.0, verbose=0, warm_start=False)\n",
    "clf8 = lgb.LGBMClassifier(objective='binary',\n",
    "                        num_leaves = 23,\n",
    "                        learning_rate=0.1,\n",
    "                        n_estimators=100)\n",
    "clf9 = lgb.LGBMClassifier(boosting_type='gbdt', colsample_bytree=0.63437546541680478,\n",
    "        learning_rate=0.1, max_bin=255, max_depth=-1, min_child_samples=10,\n",
    "        min_child_weight=5, min_split_gain=0, n_estimators=1375,\n",
    "        nthread=-1, num_leaves=316, objective='binary', reg_alpha=0,\n",
    "        reg_lambda=0, seed=0, silent=True, subsample=0.97178964806068002,\n",
    "        subsample_for_bin=50000, subsample_freq=1)\n",
    "clf10 = xgb.XGBClassifier(objective='binary:logistic',\n",
    "                        max_depth=5,\n",
    "                        learning_rate=0.1,\n",
    "                        n_estimators=100)\n",
    "clf11 = xgb.XGBClassifier(base_score=0.5, colsample_bylevel=1,\n",
    "       colsample_bytree=0.91837825878391688, gamma=0, learning_rate=0.1,\n",
    "       max_delta_step=0, max_depth=4, min_child_weight=1, missing=None,\n",
    "       n_estimators=3000, nthread=-1, objective='binary:logistic',\n",
    "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=0, silent=True,\n",
    "       subsample=0.81182984013631532)\n",
    "clf12 = KNeighborsClassifier(n_neighbors=1)\n",
    "clf13 = GaussianNB()\n",
    "clf14 = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4-fold cross validation:\n",
      "\n",
      "f1_macro: 0.78 (+/- 0.02) [RF_gini]\n",
      "f1_macro: 0.79 (+/- 0.03) [RF_entropy]\n",
      "f1_macro: 0.78 (+/- 0.03) [ET_entropy]\n",
      "f1_macro: 0.78 (+/- 0.03) [GBDT_pure]\n",
      "f1_macro: 0.81 (+/- 0.02) [GBDT_grid]\n",
      "f1_macro: 0.81 (+/- 0.02) [LGBM_pure]\n",
      "f1_macro: 0.82 (+/- 0.02) [LGBM_grid]\n",
      "f1_macro: 0.81 (+/- 0.02) [XGBOOST_pure]\n",
      "f1_macro: 0.82 (+/- 0.02) [XGBOOST_grid]\n",
      "f1_macro: 0.82 (+/- 0.03) [StackingClassifier]\n"
     ]
    }
   ],
   "source": [
    "sclf = StackingCVClassifier(classifiers=[clf1,clf2,clf4,clf5,clf7,clf8,clf9,clf10,clf11],use_probas=True,meta_classifier=lr)\n",
    "\n",
    "print('4-fold cross validation:\\n')\n",
    "\n",
    "for clf, label in zip([clf1,clf2,clf4,clf5,clf7,clf8,clf9,clf10,clf11,sclf], \n",
    "                      ['RF_gini', \n",
    "                       'RF_entropy', \n",
    "                       'ET_entropy',\n",
    "                       'GBDT_pure',\n",
    "                       'GBDT_grid',\n",
    "                       'LGBM_pure',\n",
    "                       'LGBM_grid',\n",
    "                       'XGBOOST_pure',\n",
    "                       'XGBOOST_grid',\n",
    "                       'StackingClassifier']):\n",
    "\n",
    "    scores = cross_val_score(clf, np.array(data), np.array(target), \n",
    "                            cv=4, scoring='f1_macro')\n",
    "    print(\"f1_macro: %0.2f (+/- %0.2f) [%s]\" \n",
    "          % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ham       0.88      0.94      0.91      1219\n",
      "       spam       0.82      0.67      0.74       488\n",
      "\n",
      "avg / total       0.86      0.86      0.86      1707\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test,y_train,y_test = train_test_split(np.array(data),np.array(target),test_size=0.33)\n",
    "\n",
    "binary_model = sclf.fit(X_train,y_train)\n",
    "\n",
    "y_pred = binary_model.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=[\"ham\", \"spam\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.80803818,  0.84524474,  0.84495545,  0.78610649])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
